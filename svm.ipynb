{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kolton.hauck/miniconda3/envs/python310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sub_1000.json\",\"r\") as j_file:\n",
    "    data = json.load(j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: 850\n",
      " test: 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_ratios = [0.7, 0.15, 0.15]\n",
    "\n",
    "random.shuffle(data)\n",
    "train_data, val_data, test_data = split_list(data, split_ratios)\n",
    "\n",
    "train_data += val_data\n",
    "print(f\"\"\"\n",
    "train: {len(train_data)}\n",
    " test: {len(test_data)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, encoder, device='cuda'):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Ensure the encoder is on the correct device and switch to evaluation mode\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data['embeddings'].to(device), data['label']  # Move inputs to GPU\n",
    "            feature = encoder(inputs)\n",
    "            features.append(feature.cpu().numpy())  # Move features back to CPU and convert to numpy\n",
    "            labels.append(targets.numpy())\n",
    "\n",
    "    features = np.vstack(features)\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load embeddings ###\n",
    "with open('data/text2embeddings.pkl', 'rb') as f:\n",
    "    text2embeddings = pickle.load(f)\n",
    "\n",
    "pretrained_hae = PatientAutoencoder.load_from_checkpoint(checkpoint_path='checkpoints/last.ckpt')\n",
    "encoder = nn.Sequential(\n",
    "    pretrained_hae.encounter_autoencoder,\n",
    "    pretrained_hae.patient_encoder\n",
    ")\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "### setup dataloaders ###\n",
    "train_ds, val_ds, test_ds = PatientDataset(train_data, text2embeddings), PatientDataset(val_data, text2embeddings), PatientDataset(test_data, text2embeddings)\n",
    "train_dl = DataLoader(train_ds, batch_size=20, shuffle=True, collate_fn=collate_fn, num_workers=5, persistent_workers=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=20, collate_fn=collate_fn, num_workers=5, persistent_workers=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=20, collate_fn=collate_fn, num_workers=5)\n",
    "\n",
    "# Extract features\n",
    "X_train, y_train = extract_features(train_dl, encoder)\n",
    "X_test, y_test = extract_features(test_dl, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "svm = SVC(verbose=1)\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'gamma': np.logspace(-9, 3, 13),\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Random search of parameters\n",
    "random_search = RandomizedSearchCV(estimator=svm, param_distributions=param_dist, n_iter=250, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data with the best parameters\n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
