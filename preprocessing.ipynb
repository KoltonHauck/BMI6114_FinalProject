{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook\n",
    "\n",
    "This notebook outlines the steps required to pre-process the Synthea data for use.\n",
    "\n",
    "What is given is a standard `output/` synthea folder with all tables included. What is output is a JSON file in the following (simplified) version:\n",
    "\n",
    "```JSON\n",
    "[\n",
    "    {\n",
    "        \"patient_id\": \"\",\n",
    "        \"label\": \"\",\n",
    "        \"demographics\": \"\",\n",
    "        \"encounters\": {\n",
    "            \"encounter\": [],\n",
    "            \"conditions\": [],\n",
    "            \"careplans\": []\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolton.hauck\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pa_utils import synthea_table_information, remove_text_inside_parentheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Take the raw .csv files for each table of interest and: clean, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolton.hauck\\AppData\\Local\\Temp\\1\\ipykernel_18344\\3134239183.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  encounters_df.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# read in relevant data\n",
    "synthea_path = \"output/csv/\"\n",
    "\n",
    "#### patients ####\n",
    "patients_df = pd.read_csv(synthea_path + \"patients.csv\", header=None)\n",
    "patients_df.columns = synthea_table_information[\"patients.csv\"][\"columns\"]\n",
    "\n",
    "#### encounters ####\n",
    "encounters_df = pd.read_csv(synthea_path + \"encounters.csv\", header=None)\n",
    "encounters_df.columns = synthea_table_information[\"encounters.csv\"][\"columns\"]\n",
    "encounters_df[\"Start\"] = pd.to_datetime(encounters_df[\"Start\"], utc=True)\n",
    "encounters_df[\"Stop\"] = pd.to_datetime(encounters_df[\"Stop\"], utc=True)\n",
    "encounters_df.sort_values(by=\"Start\", inplace=True)\n",
    "encounters_df.fillna(\"\", inplace=True)\n",
    "encounters_df[\"Description\"] = encounters_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "encounters_df[\"ReasonDescription\"] = encounters_df[\"ReasonDescription\"].apply(remove_text_inside_parentheses)\n",
    "\n",
    "#### careplans ####\n",
    "careplans_df = pd.read_csv(synthea_path + \"careplans.csv\", header=None, dtype=str)\n",
    "careplans_df.columns = synthea_table_information[\"careplans.csv\"][\"columns\"]\n",
    "careplans_df.drop(columns=[\"Id\", \"Start\", \"Stop\", \"Patient\", \"Code\", \"ReasonCode\"], inplace=True)\n",
    "careplans_df.fillna(\"\", inplace=True)\n",
    "careplans_df[\"Description\"] = careplans_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "careplans_df[\"ReasonDescription\"] = careplans_df[\"ReasonDescription\"].apply(remove_text_inside_parentheses)\n",
    "\n",
    "#### conditions ####\n",
    "conditions_df = pd.read_csv(synthea_path + \"conditions.csv\", header=None, dtype=str)\n",
    "conditions_df.columns = synthea_table_information[\"conditions.csv\"][\"columns\"]\n",
    "conditions_df[\"Description\"] = conditions_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "conditions_df.drop(columns=[\"Start\", \"Stop\", \"Patient\", \"Code\"], inplace=True)\n",
    "\n",
    "#### procedures ####\n",
    "procedures_df = pd.read_csv(synthea_path + \"procedures.csv\", header=None, dtype=str)\n",
    "procedures_df.columns = synthea_table_information[\"procedures.csv\"][\"columns\"]\n",
    "procedures_df[\"Start\"] = pd.to_datetime(procedures_df[\"Start\"], utc=True)\n",
    "procedures_df[\"Stop\"] = pd.to_datetime(procedures_df[\"Stop\"], utc=True)\n",
    "procedures_df.sort_values(by=\"Start\", inplace=True)\n",
    "procedures_df.fillna(\"\", inplace=True)\n",
    "procedures_df[\"Description\"] = procedures_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "procedures_df[\"ReasonDescription\"] = procedures_df[\"ReasonDescription\"].apply(remove_text_inside_parentheses)\n",
    "# drop later -->> # procedures_df.drop(columns=[\"Start\", \"Stop\", \"Patient\", \"Code\", \"ReasonCode\", \"Base_Cost\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmp1\n",
    "\n",
    "Take the filtered csv (df), turn into JSON format -> faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careplans\n",
    "grouped_tmp = careplans_df.groupby(\"Encounter\").apply(lambda x: x.drop(\"Encounter\", axis=1).to_dict(orient=\"records\")).to_dict()\n",
    "\n",
    "with open(\"data_processed/tmp1/careplans_1.json\", \"w\") as j_file:\n",
    "    json.dump(grouped_tmp, j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "grouped_tmp = conditions_df.groupby(\"Encounter\").apply(lambda x: x.drop(\"Encounter\", axis=1).to_dict(orient=\"records\")).to_dict()\n",
    "\n",
    "with open(\"data_processed/tmp1/conditions_1.json\", \"w\") as j_file:\n",
    "    json.dump(grouped_tmp, j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedures\n",
    "grouped_tmp = procedures_df.drop(columns=[\"\"]).groupby(\"Encounter\").apply(lambda x: x.drop(\"Encounter\", axis=1).to_dict(orient=\"records\")).to_dict()\n",
    "\n",
    "with open(\"data_processed/tmp1/procedures_1.json\", \"w\") as j_file:\n",
    "    json.dump(grouped_tmp, j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7375831/7375831 [09:10<00:00, 13404.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# procedures\n",
    "\n",
    "grouped_tmp = {}\n",
    "\n",
    "for index, row in tqdm(procedures_df.iterrows(), total=procedures_df.shape[0]):\n",
    "    curr_id = row[\"Encounter\"]\n",
    "\n",
    "    curr_dict = {\n",
    "        \"Description\": row[\"Description\"],\n",
    "        \"ReasonDescription\": row[\"ReasonDescription\"]\n",
    "    }\n",
    "\n",
    "    if curr_id in grouped_tmp:\n",
    "        grouped_tmp[curr_id].append(curr_dict)\n",
    "    else:\n",
    "        grouped_tmp[curr_id] = [curr_dict]\n",
    "\n",
    "with open(\"data_processed/tmp1/procedures_1.json\", \"w\") as j_file:\n",
    "    json.dump(grouped_tmp, j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800643/3800643 [03:46<00:00, 16768.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# encounters\n",
    "\n",
    "grouped_tmp = {}\n",
    "\n",
    "for index, row in tqdm(encounters_df.iterrows(), total=encounters_df.shape[0]):\n",
    "    grouped_tmp[row[\"Id\"]] = {\n",
    "        \"Description\": row[\"Description\"],\n",
    "        \"ReasonDescription\": row[\"ReasonDescription\"]\n",
    "    }\n",
    "\n",
    "with open(\"data_processed/tmp1/encounters_1.json\", \"w\") as j_file:\n",
    "    json.dump(grouped_tmp, j_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmp 2 - old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. given single encounter id\n",
    "2. get encounter level description adn reasondescription\n",
    "3. get all descriptions for conditions associated with encounter id\n",
    "4. get all description, reasondescriptions for all careplans assocaited with encounter id\n",
    "5. get all description, reasondescriptions for all procedures associated with encounter id\n",
    "\"\"\"\n",
    "\n",
    "def load_enc_dict(encounter_id):\n",
    "    curr_enc = {}\n",
    "\n",
    "    encounter_dict = encounters_df[encounters_df.Id == encounter_id].iloc[0].to_dict()\n",
    "    curr_enc[\"encounter\"] = {\n",
    "        \"Description\": encounter_dict[\"Description\"],\n",
    "        \"ReasonDescription\": encounter_dict[\"ReasonDescription\"]\n",
    "    }\n",
    "\n",
    "    encounter_conditions_df = conditions_df[conditions_df.Encounter == encounter_id]\n",
    "    curr_enc[\"conditions\"] = encounter_conditions_df[[\"Description\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    encounter_careplans_df = careplans_df[careplans_df.Encounter == encounter_id]\n",
    "    curr_enc[\"careplans\"] = encounter_careplans_df[[\"Description\", \"ReasonDescription\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    encounter_procedures_df = procedures_df[procedures_df.Encounter == encounter_id]\n",
    "    curr_enc[\"procedures\"] = encounter_procedures_df[[\"Description\", \"ReasonDescription\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    return curr_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. given a single patient id and it's label and date of poi\n",
    "2. get encounter df\n",
    "3. if in-class: drop later encounters (past poi)\n",
    "4. get patient level data\n",
    "5. append encounter level data\n",
    "\"\"\"\n",
    "\n",
    "def get_patient_dict(patient_id, label, date_poi):\n",
    "    patient_dict = patients_df[patients_df.Id == patient_id].iloc[0].to_dict()\n",
    "\n",
    "    patient_encounters_df = encounters_df[encounters_df.Patient == patient_id]\n",
    "    \n",
    "    if date_poi:\n",
    "        patient_encounters_df = patient_encounters_df[patient_encounters_df[\"Start\"] <= date_poi] # drop encounters after poi\n",
    "\n",
    "    #@todo1: sort patient_encounters_df\n",
    "\n",
    "    curr_pat = {\n",
    "        \"patient_id\": patient_dict[\"Id\"],\n",
    "        \"label\": label,\n",
    "        \"lat\": patient_dict[\"Lat\"],\n",
    "        \"lon\": patient_dict[\"Lon\"],\n",
    "        \"encounters\": []\n",
    "    }\n",
    "\n",
    "    encounter_ids = patient_encounters_df.Id.to_list()\n",
    "    for encounter_id in encounter_ids:\n",
    "        curr_pat[\"encounters\"].append(load_enc_dict(encounter_id))\n",
    "\n",
    "    return curr_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. get all patient ids\n",
    "2. get the patients with poi\n",
    "    2.1 retain only the encounters up to the date of poi\n",
    "    2.2 also get which patients are in class\n",
    "3. iter over each patient\n",
    "    3.1 for each patient: get the 'dict' object\n",
    "\"\"\"\n",
    "\n",
    "patient_ids = patients_df.Id.to_list()[:100] # subset to test\n",
    "\n",
    "poi = \"Plain chest X-ray\"\n",
    "patients_in_class = procedures_df[procedures_df[\"Description\"] == poi][[\"Patient\", \"Start\"]].drop_duplicates()\n",
    "procedures_df.drop(columns=[\"Start\", \"Stop\", \"Patient\", \"Code\", \"ReasonCode\", \"Base_Cost\"], inplace=True)\n",
    "\n",
    "idx = patients_in_class.groupby(\"Patient\")[\"Start\"].idxmax() # focus on the last poi given to the patient (eg patient can have multiple chest x-rays )\n",
    "patients_in_class_last_procedure = patients_in_class.loc[idx]\n",
    "\n",
    "patients_dicts = []\n",
    "\n",
    "for patient_id in tqdm(patient_ids):\n",
    "    label = patient_id in patients_in_class.Patient.to_list()\n",
    "    date_poi = patients_in_class_last_procedure[patients_in_class_last_procedure[\"Patient\"] == patient_id]\n",
    "\n",
    "    if date_poi.empty:\n",
    "        date_poi = None\n",
    "    else:\n",
    "        date_poi = date_poi.iloc[0].Start\n",
    "        print(date_poi)\n",
    "\n",
    "    patients_dicts.append(get_patient_dict(patient_id, label, date_poi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded careplans\n",
      "loaded conditions\n",
      "loaded encounters\n",
      "loaded procedures\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_processed/tmp1/careplans_1.json\", \"r\") as j_file:\n",
    "    careplans_json = json.load(j_file)\n",
    "    print(\"loaded careplans\")\n",
    "\n",
    "with open(\"data_processed/tmp1/conditions_1.json\", \"r\") as j_file:\n",
    "    conditions_json = json.load(j_file)\n",
    "    print(\"loaded conditions\")\n",
    "\n",
    "with open(\"data_processed/tmp1/encounters_1.json\", \"r\") as j_file:\n",
    "    encounters_json = json.load(j_file)\n",
    "    print(\"loaded encounters\")\n",
    "\n",
    "with open(\"data_processed/tmp1/procedures_1.json\", \"r\") as j_file:\n",
    "    procedures_json = json.load(j_file)\n",
    "    print(\"loaded procedures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded patients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolton.hauck\\AppData\\Local\\Temp\\1\\ipykernel_16356\\893348829.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  encounters_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded encounters\n",
      "loaded procedures\n"
     ]
    }
   ],
   "source": [
    "synthea_path = \"output/csv/\"\n",
    "\n",
    "#### patients ####\n",
    "patients_df = pd.read_csv(synthea_path + \"patients.csv\", header=None)\n",
    "patients_df.columns = synthea_table_information[\"patients.csv\"][\"columns\"]\n",
    "print(\"loaded patients\")\n",
    "\n",
    "#### encounters ####\n",
    "encounters_df = pd.read_csv(synthea_path + \"encounters.csv\", header=None)\n",
    "encounters_df.columns = synthea_table_information[\"encounters.csv\"][\"columns\"]\n",
    "encounters_df[\"Start\"] = pd.to_datetime(encounters_df[\"Start\"], utc=True)\n",
    "encounters_df[\"Stop\"] = pd.to_datetime(encounters_df[\"Stop\"], utc=True)\n",
    "encounters_df.sort_values(by=\"Start\", inplace=True)\n",
    "encounters_df.fillna(\"\", inplace=True)\n",
    "encounters_df[\"Description\"] = encounters_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "encounters_df[\"ReasonDescription\"] = encounters_df[\"ReasonDescription\"].apply(remove_text_inside_parentheses)\n",
    "print(\"loaded encounters\")\n",
    "\n",
    "#### procedures ####\n",
    "procedures_df = pd.read_csv(synthea_path + \"procedures.csv\", header=None, dtype=str)\n",
    "procedures_df.columns = synthea_table_information[\"procedures.csv\"][\"columns\"]\n",
    "procedures_df[\"Start\"] = pd.to_datetime(procedures_df[\"Start\"], utc=True)\n",
    "procedures_df[\"Stop\"] = pd.to_datetime(procedures_df[\"Stop\"], utc=True)\n",
    "procedures_df.sort_values(by=\"Start\", inplace=True)\n",
    "procedures_df.fillna(\"\", inplace=True)\n",
    "procedures_df[\"Description\"] = procedures_df[\"Description\"].apply(remove_text_inside_parentheses)\n",
    "procedures_df[\"ReasonDescription\"] = procedures_df[\"ReasonDescription\"].apply(remove_text_inside_parentheses)\n",
    "print(\"loaded procedures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. given single encounter id\n",
    "2. get encounter level description adn reasondescription\n",
    "3. get all descriptions for conditions associated with encounter id\n",
    "4. get all description, reasondescriptions for all careplans assocaited with encounter id\n",
    "5. get all description, reasondescriptions for all procedures associated with encounter id\n",
    "\"\"\"\n",
    "\n",
    "def load_enc_dict(encounter_id):\n",
    "    curr_enc = {}\n",
    "\n",
    "    # enc -> description / reasondescription - one\n",
    "    curr_enc[\"encounter\"] = encounters_json[encounter_id]\n",
    "\n",
    "    # enc conditions -> description - multiple\n",
    "    curr_enc[\"conditions\"] = conditions_json.get(encounter_id, [{\"Description\": \"\"}])\n",
    "\n",
    "    # enc careplans -> description / reasondescription - multiple\n",
    "    curr_enc[\"careplans\"] = careplans_json.get(encounter_id, [{\"Description\": \"\", \"ReasonDescription\": \"\"}])\n",
    "\n",
    "    # enc procedures -> description / reasondescription - multiple\n",
    "    curr_enc[\"procedures\"] = procedures_json.get(encounter_id, [{\"Description\": \"\", \"ReasonDescription\": \"\"}])\n",
    "\n",
    "    return curr_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. given a single patient id and it's label and date of poi\n",
    "2. get encounter df\n",
    "3. if in-class: drop later encounters (past poi)\n",
    "4. get patient level data\n",
    "5. append encounter level data\n",
    "\"\"\"\n",
    "\n",
    "def get_patient_dict(patient_id, label, date_poi):\n",
    "    patient_dict = patients_df[patients_df.Id == patient_id].iloc[0].to_dict()\n",
    "\n",
    "    patient_encounters_df = encounters_df[encounters_df.Patient == patient_id]\n",
    "    \n",
    "    if date_poi:\n",
    "        patient_encounters_df = patient_encounters_df[patient_encounters_df[\"Start\"] <= date_poi] # drop encounters after poi\n",
    "\n",
    "    #@todo1: sort patient_encounters_df\n",
    "\n",
    "    curr_pat = {\n",
    "        \"patient_id\": patient_dict[\"Id\"],\n",
    "        \"label\": label,\n",
    "        \"lat\": patient_dict[\"Lat\"],\n",
    "        \"lon\": patient_dict[\"Lon\"],\n",
    "        \"encounters\": []\n",
    "    }\n",
    "\n",
    "    encounter_ids = patient_encounters_df.Id.to_list()\n",
    "    for encounter_id in encounter_ids:\n",
    "        curr_pat[\"encounters\"].append(load_enc_dict(encounter_id))\n",
    "\n",
    "    return curr_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. get all patient ids\n",
    "2. get the patients with poi\n",
    "    2.1 retain only the encounters up to the date of poi\n",
    "    2.2 also get which patients are in class\n",
    "3. iter over each patient\n",
    "    3.1 for each patient: get the 'dict' object\n",
    "\"\"\"\n",
    "\n",
    "patient_ids = patients_df.Id.to_list()\n",
    "\n",
    "poi = \"Plain chest X-ray\"\n",
    "patients_in_class = procedures_df[procedures_df[\"Description\"] == poi][[\"Patient\", \"Start\"]].drop_duplicates()\n",
    "#procedures_dropped = procedures_df.drop(columns=[\"Start\", \"Stop\", \"Patient\", \"Code\", \"ReasonCode\", \"Base_Cost\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 48443/57728 [3:41:11<1:01:17,  2.52it/s] "
     ]
    }
   ],
   "source": [
    "# v1\n",
    "idx = patients_in_class.groupby(\"Patient\")[\"Start\"].idxmax() # focus on the last poi given to the patient (eg patient can have multiple chest x-rays )\n",
    "patients_in_class_last_procedure = patients_in_class.loc[idx]\n",
    "\n",
    "patients_dicts = []\n",
    "\n",
    "for patient_id in tqdm(patient_ids):\n",
    "    label = patient_id in patients_in_class.Patient.to_list()\n",
    "    date_poi = patients_in_class_last_procedure[patients_in_class_last_procedure[\"Patient\"] == patient_id]\n",
    "\n",
    "    if date_poi.empty:\n",
    "        date_poi = None\n",
    "    else:\n",
    "        date_poi = date_poi.iloc[0].Start\n",
    "\n",
    "    patients_dicts.append(get_patient_dict(patient_id, label, date_poi))\n",
    "\n",
    "with open(\"data_processed/json/all.json\", \"w\") as j_file:\n",
    "    json.dump(patients_dicts, j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57728/57728 [3:44:42<00:00,  4.28it/s]  \n"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a function get_patient_dict that builds the dictionary for each patient\n",
    "\n",
    "# Preprocess to avoid repeated operations inside the loop\n",
    "patient_set = set(patients_in_class[\"Patient\"])\n",
    "\n",
    "idx = patients_in_class.groupby(\"Patient\")[\"Start\"].idxmax() # focus on the last poi given to the patient (eg patient can have multiple chest x-rays )\n",
    "patients_in_class_last_procedure = patients_in_class.loc[idx]\n",
    "last_procedure_dict = patients_in_class_last_procedure.set_index(\"Patient\")[\"Start\"].to_dict()\n",
    "\n",
    "patients_dicts = []\n",
    "chunk_size=500\n",
    "\n",
    "for patient_id in tqdm(patient_ids):\n",
    "    label = patient_id in patient_set\n",
    "    date_poi = last_procedure_dict.get(patient_id, None)\n",
    "\n",
    "    patients_dicts.append(get_patient_dict(patient_id, label, date_poi))\n",
    "\n",
    "    # Optionally, write to file in chunks to avoid memory overflow\n",
    "    # if len(patients_dicts) >= chunk_size:\n",
    "    #     with open(\"data_processed/json/all.json\", \"a\") as file:\n",
    "    #         json.dump(patients_dicts, file)\n",
    "    #     patients_dicts = []  # Reset the list for the next chunk\n",
    "\n",
    "# Don't forget to write the last chunk\n",
    "if patients_dicts:\n",
    "    # with open(\"data_processed/json/all.json\", \"a\") as file:\n",
    "    with open(\"data_processed/json/all.json\", \"w\") as file:\n",
    "        json.dump(patients_dicts, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General DF\n",
    "\n",
    "This will take the 'all.json' file produced in Tmp 2 step, vectorize all descriptions and get the data into a tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_processed/json/all.json\", \"r\") as j_file:\n",
    "    patients_dict = json.load(j_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57728/57728 [01:37<00:00, 589.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings\n",
    "\n",
    "# get set of all descriptions / reasondescriptions\n",
    "texts = set()\n",
    "\n",
    "for patient in tqdm(patients_dict):\n",
    "    for encounter in patient[\"encounters\"]:\n",
    "        texts.add(encounter[\"encounter\"][\"Description\"]) # append enc description\n",
    "        texts.add(encounter[\"encounter\"][\"ReasonDescription\"]) # append enc reasondescription\n",
    "\n",
    "        texts = texts | set([_[\"Description\"] for _ in encounter[\"conditions\"]]) # condition desc\n",
    "\n",
    "        texts = texts | set([_[\"Description\"] for _ in encounter[\"careplans\"]]) # careplan descs\n",
    "\n",
    "        texts = texts | set([_[\"ReasonDescription\"] for _ in encounter[\"careplans\"]]) # careplan reas descs\n",
    "\n",
    "        texts = texts | set([_[\"Description\"] for _ in encounter[\"procedures\"]]) # proc descs\n",
    "\n",
    "        texts = texts | set([_[\"ReasonDescription\"] for _ in encounter[\"procedures\"]]) # proc reas descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_descriptions(descriptions, model_directory, batch_size=32):\n",
    "    \"\"\" Given a dictionary of SNOMED code:description k:v pairs, and an embedding model (IE BERT), returns a dict of description:embedding of each code\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained(model_directory)\n",
    "    model.eval()  # Put the model in \"evaluation\" mode, which turns off dropout\n",
    "    print(f\"model loaded | generating embeddings with bs={batch_size}\")\n",
    "\n",
    "    inputs = tokenizer(descriptions, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "    # Assuming you have enough memory, process the entire list in batches\n",
    "    embeddings = []\n",
    "\n",
    "    # Process in batches with tqdm for progress tracking\n",
    "    for i in tqdm(range(0, len(descriptions), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch = inputs[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        # Extract pooled output embeddings\n",
    "        batch_embeddings = outputs.pooler_output\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    # Concatenate batched embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    return {descriptions[i]: embeddings[i] for i in range(len(descriptions))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MPNetTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type mpnet to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at models/BioLORD-2023/ and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded | generating embeddings with bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# create dict of embeddings\n",
    "embeddings = embed_descriptions(list(texts), \"models/BioLORD-2023/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Encounter for symptom',\n",
       "   'ReasonDescription': 'Streptococcal sore throat'},\n",
       "  'conditions': [{'Description': 'Streptococcal sore throat'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': 'Medication Reconciliation',\n",
       "    'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Emergency room admission',\n",
       "   'ReasonDescription': 'Fracture of bone'},\n",
       "  'conditions': [{'Description': 'Fracture of bone'},\n",
       "   {'Description': 'Fracture of ankle'}],\n",
       "  'careplans': [{'Description': 'Fracture care',\n",
       "    'ReasonDescription': 'Fracture of ankle'}],\n",
       "  'procedures': [{'Description': 'Bone immobilization',\n",
       "    'ReasonDescription': 'Fracture of ankle'},\n",
       "   {'Description': 'Ankle X-ray', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Encounter for check up',\n",
       "   'ReasonDescription': 'Fracture of ankle'},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': 'Medication review due'}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]},\n",
       " {'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       "  'conditions': [{'Description': ''}],\n",
       "  'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       "  'procedures': [{'Description': '', 'ReasonDescription': ''}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients_dict[0][\"encounters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = patients_dict[0][\"encounters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encounter': {'Description': 'Well child visit', 'ReasonDescription': ''},\n",
       " 'conditions': [{'Description': 'Medication review due'}],\n",
       " 'careplans': [{'Description': '', 'ReasonDescription': ''}],\n",
       " 'procedures': [{'Description': 'Medication Reconciliation',\n",
       "   'ReasonDescription': ''}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_enc_feats(list_of_vecs):\n",
    "    return np.mean(list_of_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encounters(encounters, embeddings, agg_func=average_enc_feats):\n",
    "\n",
    "    encounter_vecs = []\n",
    "\n",
    "    for encounter in encounters:\n",
    "        curr_enc_vec = []\n",
    "\n",
    "        curr_enc_vec.extend(embeddings[encounter[\"encounter\"][\"Description\"]]) # append enc description\n",
    "        curr_enc_vec.extend(embeddings[encounter[\"encounter\"][\"ReasonDescription\"]]) # append enc reasondescription\n",
    "\n",
    "        curr_cond_descs_ = [embeddings[_[\"Description\"]] for _ in encounter[\"conditions\"]]\n",
    "        curr_enc_vec.extend(agg_func(curr_cond_descs_)) # append condition description aggregated\n",
    "\n",
    "        curr_carep_descs_ = [embeddings[_[\"Description\"]] for _ in encounter[\"careplans\"]]\n",
    "        curr_enc_vec.extend(agg_func(curr_carep_descs_)) # append careplan description aggregated\n",
    "\n",
    "        curr_carep_rdescs_ = [embeddings[_[\"ReasonDescription\"]] for _ in encounter[\"careplans\"]]\n",
    "        curr_enc_vec.extend(agg_func(curr_carep_rdescs_)) # append careplan reasondescription aggregated\n",
    "\n",
    "        curr_proc_descs_ = [embeddings[_[\"Description\"]] for _ in encounter[\"procedures\"]]\n",
    "        curr_enc_vec.extend(agg_func(curr_proc_descs_)) # append proc description aggregated\n",
    "\n",
    "        curr_proc_rdescs_ = [embeddings[_[\"ReasonDescription\"]] for _ in encounter[\"procedures\"]]\n",
    "        curr_enc_vec.extend(agg_func(curr_proc_rdescs_)) # append proc reasondescription aggregated\n",
    "        \n",
    "        encounter_vecs.append(curr_enc_vec)\n",
    "\n",
    "    return average_enc_feats(encounter_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57728/57728 [14:11:00<00:00,  1.13it/s]   \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    patients_done = pd.read_csv(\"flattened_dataset.csv\", header=None)[0].to_list()\n",
    "except:\n",
    "    patients_done = []\n",
    "\n",
    "with open(\"flattened_dataset.csv\", \"a\") as txt_file:\n",
    "    # iter over each patient\n",
    "    for patient in tqdm(patients_dict):\n",
    "        if patient[\"patient_id\"] in patients_done:\n",
    "            continue\n",
    "\n",
    "        curr_pat = []\n",
    "        curr_pat.append(patient[\"patient_id\"]) # patient id (index)\n",
    "        curr_pat.append(int(patient[\"label\"])) # label (signal / target)\n",
    "        curr_pat.append(patient[\"lat\"]) # lat of patient\n",
    "        curr_pat.append(patient[\"lon\"]) # lon of patient\n",
    "\n",
    "        curr_pat.extend(load_encounters(patient[\"encounters\"], embeddings))\n",
    "\n",
    "        txt_file.write(\",\".join([str(item) for item in curr_pat]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57728"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    patients_done = pd.read_csv(\"flattened_dataset.csv\", header=None)[0].to_list()\n",
    "except:\n",
    "    patients_done = []\n",
    "\n",
    "len(patients_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5370</th>\n",
       "      <th>5371</th>\n",
       "      <th>5372</th>\n",
       "      <th>5373</th>\n",
       "      <th>5374</th>\n",
       "      <th>5375</th>\n",
       "      <th>5376</th>\n",
       "      <th>5377</th>\n",
       "      <th>5378</th>\n",
       "      <th>5379</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>094e2672-7c1b-eba8-3405-20be988a3811</td>\n",
       "      <td>0</td>\n",
       "      <td>40.873811</td>\n",
       "      <td>-109.431528</td>\n",
       "      <td>-0.031033</td>\n",
       "      <td>0.289719</td>\n",
       "      <td>0.027958</td>\n",
       "      <td>-0.227295</td>\n",
       "      <td>-0.150596</td>\n",
       "      <td>0.169259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330483</td>\n",
       "      <td>-0.023646</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>0.180497</td>\n",
       "      <td>-0.034652</td>\n",
       "      <td>-0.113333</td>\n",
       "      <td>0.324499</td>\n",
       "      <td>0.288531</td>\n",
       "      <td>-0.020623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6c8fad2-773f-4a81-7737-d4ded883f521</td>\n",
       "      <td>0</td>\n",
       "      <td>40.589988</td>\n",
       "      <td>-111.721005</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>0.249736</td>\n",
       "      <td>-0.092530</td>\n",
       "      <td>-0.167370</td>\n",
       "      <td>-0.073288</td>\n",
       "      <td>0.214831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330803</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>0.022981</td>\n",
       "      <td>-0.019290</td>\n",
       "      <td>0.180746</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>-0.113931</td>\n",
       "      <td>0.324178</td>\n",
       "      <td>0.289040</td>\n",
       "      <td>-0.020862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6e18deb0-7724-65e6-3b62-c052d30cd63e</td>\n",
       "      <td>0</td>\n",
       "      <td>40.697847</td>\n",
       "      <td>-112.205052</td>\n",
       "      <td>-0.027772</td>\n",
       "      <td>0.287580</td>\n",
       "      <td>0.018022</td>\n",
       "      <td>-0.225007</td>\n",
       "      <td>-0.141567</td>\n",
       "      <td>0.172837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330576</td>\n",
       "      <td>-0.023856</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>-0.018447</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-0.034598</td>\n",
       "      <td>-0.113134</td>\n",
       "      <td>0.324633</td>\n",
       "      <td>0.288424</td>\n",
       "      <td>-0.021169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3829fbc5-cfb0-341d-b251-27b25ab7cc41</td>\n",
       "      <td>0</td>\n",
       "      <td>40.620239</td>\n",
       "      <td>-111.918419</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>0.262305</td>\n",
       "      <td>-0.105305</td>\n",
       "      <td>-0.211218</td>\n",
       "      <td>-0.131809</td>\n",
       "      <td>0.188591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325680</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>-0.013930</td>\n",
       "      <td>0.180542</td>\n",
       "      <td>-0.037471</td>\n",
       "      <td>-0.116612</td>\n",
       "      <td>0.320529</td>\n",
       "      <td>0.285655</td>\n",
       "      <td>-0.016130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58460ba5-986e-89eb-3d53-cc3133658f46</td>\n",
       "      <td>0</td>\n",
       "      <td>40.686387</td>\n",
       "      <td>-111.985047</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>0.235343</td>\n",
       "      <td>-0.095788</td>\n",
       "      <td>-0.169949</td>\n",
       "      <td>-0.086596</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328867</td>\n",
       "      <td>-0.025385</td>\n",
       "      <td>0.023512</td>\n",
       "      <td>-0.017824</td>\n",
       "      <td>0.180867</td>\n",
       "      <td>-0.034692</td>\n",
       "      <td>-0.114863</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.287333</td>\n",
       "      <td>-0.019064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>394d0493-f050-60fc-80ea-a3dc0afa31fc</td>\n",
       "      <td>0</td>\n",
       "      <td>33.954177</td>\n",
       "      <td>-86.973280</td>\n",
       "      <td>-0.027428</td>\n",
       "      <td>0.285705</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>-0.217754</td>\n",
       "      <td>-0.138904</td>\n",
       "      <td>0.180988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327324</td>\n",
       "      <td>-0.027429</td>\n",
       "      <td>0.028610</td>\n",
       "      <td>-0.015873</td>\n",
       "      <td>0.179260</td>\n",
       "      <td>-0.035154</td>\n",
       "      <td>-0.115409</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>0.284266</td>\n",
       "      <td>-0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3f4c1c8b-d157-9bbe-64d6-9f1ef6689f80</td>\n",
       "      <td>0</td>\n",
       "      <td>32.496954</td>\n",
       "      <td>-85.362757</td>\n",
       "      <td>-0.035861</td>\n",
       "      <td>0.300091</td>\n",
       "      <td>0.034468</td>\n",
       "      <td>-0.225611</td>\n",
       "      <td>-0.155050</td>\n",
       "      <td>0.164434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330576</td>\n",
       "      <td>-0.023856</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>-0.018447</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-0.034598</td>\n",
       "      <td>-0.113134</td>\n",
       "      <td>0.324632</td>\n",
       "      <td>0.288424</td>\n",
       "      <td>-0.021169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>e6ff88a7-b53c-84ce-3fe9-2ef77a14e1e6</td>\n",
       "      <td>0</td>\n",
       "      <td>33.548084</td>\n",
       "      <td>-86.992118</td>\n",
       "      <td>0.027519</td>\n",
       "      <td>0.268755</td>\n",
       "      <td>-0.097893</td>\n",
       "      <td>-0.177759</td>\n",
       "      <td>-0.036013</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311539</td>\n",
       "      <td>-0.023682</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>-0.007975</td>\n",
       "      <td>0.184834</td>\n",
       "      <td>-0.032264</td>\n",
       "      <td>-0.117677</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>0.281175</td>\n",
       "      <td>-0.005406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>24e3419e-2c5f-89e2-d06b-c5d7c62c3cf3</td>\n",
       "      <td>0</td>\n",
       "      <td>33.440866</td>\n",
       "      <td>-88.030875</td>\n",
       "      <td>0.024499</td>\n",
       "      <td>0.266799</td>\n",
       "      <td>-0.094639</td>\n",
       "      <td>-0.164047</td>\n",
       "      <td>-0.038105</td>\n",
       "      <td>0.201948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316801</td>\n",
       "      <td>-0.036090</td>\n",
       "      <td>0.025491</td>\n",
       "      <td>-0.018434</td>\n",
       "      <td>0.187608</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>-0.117916</td>\n",
       "      <td>0.309503</td>\n",
       "      <td>0.279871</td>\n",
       "      <td>-0.013216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1f10bc05-2c9b-32bf-4e50-05bb9517edfd</td>\n",
       "      <td>0</td>\n",
       "      <td>33.457846</td>\n",
       "      <td>-86.895428</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>-0.088399</td>\n",
       "      <td>-0.152823</td>\n",
       "      <td>-0.039617</td>\n",
       "      <td>0.188002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330669</td>\n",
       "      <td>-0.023677</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>-0.018794</td>\n",
       "      <td>0.180264</td>\n",
       "      <td>-0.034680</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>0.324445</td>\n",
       "      <td>0.288678</td>\n",
       "      <td>-0.021042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5380 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0     1          2           3     \\\n",
       "0   094e2672-7c1b-eba8-3405-20be988a3811     0  40.873811 -109.431528   \n",
       "1   c6c8fad2-773f-4a81-7737-d4ded883f521     0  40.589988 -111.721005   \n",
       "2   6e18deb0-7724-65e6-3b62-c052d30cd63e     0  40.697847 -112.205052   \n",
       "3   3829fbc5-cfb0-341d-b251-27b25ab7cc41     0  40.620239 -111.918419   \n",
       "4   58460ba5-986e-89eb-3d53-cc3133658f46     0  40.686387 -111.985047   \n",
       "..                                   ...   ...        ...         ...   \n",
       "95  394d0493-f050-60fc-80ea-a3dc0afa31fc     0  33.954177  -86.973280   \n",
       "96  3f4c1c8b-d157-9bbe-64d6-9f1ef6689f80     0  32.496954  -85.362757   \n",
       "97  e6ff88a7-b53c-84ce-3fe9-2ef77a14e1e6     0  33.548084  -86.992118   \n",
       "98  24e3419e-2c5f-89e2-d06b-c5d7c62c3cf3     0  33.440866  -88.030875   \n",
       "99  1f10bc05-2c9b-32bf-4e50-05bb9517edfd     0  33.457846  -86.895428   \n",
       "\n",
       "        4         5         6         7         8         9     ...      5370  \\\n",
       "0  -0.031033  0.289719  0.027958 -0.227295 -0.150596  0.169259  ... -0.330483   \n",
       "1   0.035127  0.249736 -0.092530 -0.167370 -0.073288  0.214831  ... -0.330803   \n",
       "2  -0.027772  0.287580  0.018022 -0.225007 -0.141567  0.172837  ... -0.330576   \n",
       "3   0.013951  0.262305 -0.105305 -0.211218 -0.131809  0.188591  ... -0.325680   \n",
       "4   0.047515  0.235343 -0.095788 -0.169949 -0.086596  0.217631  ... -0.328867   \n",
       "..       ...       ...       ...       ...       ...       ...  ...       ...   \n",
       "95 -0.027428  0.285705  0.016409 -0.217754 -0.138904  0.180988  ... -0.327324   \n",
       "96 -0.035861  0.300091  0.034468 -0.225611 -0.155050  0.164434  ... -0.330576   \n",
       "97  0.027519  0.268755 -0.097893 -0.177759 -0.036013  0.192500  ... -0.311539   \n",
       "98  0.024499  0.266799 -0.094639 -0.164047 -0.038105  0.201948  ... -0.316801   \n",
       "99  0.020020  0.275031 -0.088399 -0.152823 -0.039617  0.188002  ... -0.330669   \n",
       "\n",
       "        5371      5372      5373      5374      5375      5376      5377  \\\n",
       "0  -0.023646  0.022632 -0.018505  0.180497 -0.034652 -0.113333  0.324499   \n",
       "1  -0.023421  0.022981 -0.019290  0.180746 -0.034797 -0.113931  0.324178   \n",
       "2  -0.023856  0.022236 -0.018447  0.179926 -0.034598 -0.113134  0.324633   \n",
       "3  -0.028446  0.028082 -0.013930  0.180542 -0.037471 -0.116612  0.320529   \n",
       "4  -0.025385  0.023512 -0.017824  0.180867 -0.034692 -0.114863  0.322222   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.027429  0.028610 -0.015873  0.179260 -0.035154 -0.115409  0.322600   \n",
       "96 -0.023856  0.022236 -0.018447  0.179926 -0.034598 -0.113134  0.324632   \n",
       "97 -0.023682  0.027404 -0.007975  0.184834 -0.032264 -0.117677  0.304834   \n",
       "98 -0.036090  0.025491 -0.018434  0.187608 -0.036414 -0.117916  0.309503   \n",
       "99 -0.023677  0.022543 -0.018794  0.180264 -0.034680 -0.113462  0.324445   \n",
       "\n",
       "        5378      5379  \n",
       "0   0.288531 -0.020623  \n",
       "1   0.289040 -0.020862  \n",
       "2   0.288424 -0.021169  \n",
       "3   0.285655 -0.016130  \n",
       "4   0.287333 -0.019064  \n",
       "..       ...       ...  \n",
       "95  0.284266 -0.018300  \n",
       "96  0.288424 -0.021169  \n",
       "97  0.281175 -0.005406  \n",
       "98  0.279871 -0.013216  \n",
       "99  0.288678 -0.021042  \n",
       "\n",
       "[100 rows x 5380 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"flattened_dataset.csv\", header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
